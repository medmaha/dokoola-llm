version: "3.8"

services:
  llm-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-api
    ports:
      - 8080:8000
    env_file:
      - .env
    volumes:
      # Mount source code for development (comment out for production)
      - .:/app
      # Mount logs directory if needed
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: [CMD, curl, -f, http://localhost:8000/api/health/]
      interval: 90s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '0.3'
          memory: ${LLM_API_MEMORY_LIMIT:-512M}
        reservations:
          cpus: '0.15'
          memory: ${LLM_API_MEMORY_RESERVATION:-256M}

  # nginx:
  #   image: nginx:alpine
  #   container_name: llm-api-nginx
  #   ports:
  #     - "80:80"
  #     - "443:443"
  #   volumes:
  #     - ./nginx.conf:/etc/nginx/nginx.conf
  #   depends_on:
  #     - llm-api
  #   restart: unless-stopped

networks:
  default:
    name: llm-api-network
